{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Image_Captioning.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN02NHZQvELjItavLVE2H2g"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"233f374cd0fa4987bdbe2e3710b30eba":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_583d49f038cf49bfb502e4a75767c899","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e5e0e5d2809747e499a40a1f9f768f05","IPY_MODEL_d5fb5e589d0f44bfaf7b79dd147a4aff"]}},"583d49f038cf49bfb502e4a75767c899":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e5e0e5d2809747e499a40a1f9f768f05":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b4ba0f0a5cd045ecb0faba1cb25520c8","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":8091,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":8091,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_39a90fe32f244810a10e9d3437a99641"}},"d5fb5e589d0f44bfaf7b79dd147a4aff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ca8e1f7b2af64d3991fbc2a833eb5cfc","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 8091/8091 [04:00&lt;00:00, 33.59it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_75d7e5c8169349cab6b02cf5020debdc"}},"b4ba0f0a5cd045ecb0faba1cb25520c8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"39a90fe32f244810a10e9d3437a99641":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ca8e1f7b2af64d3991fbc2a833eb5cfc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"75d7e5c8169349cab6b02cf5020debdc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ca74AWTggJy1","colab_type":"text"},"source":["# Download and Unzip the datasets"]},{"cell_type":"code","metadata":{"id":"pDKrPrXribGR","colab_type":"code","colab":{}},"source":["!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UKkBHIz8iepH","colab_type":"code","colab":{}},"source":["!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cumqatOWikFg","colab_type":"code","colab":{}},"source":["!unzip Flickr8k_Dataset.zip\n","\n","!unzip Flickr8k_text.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QkBu57nCg7Vl","colab_type":"text"},"source":["# Import alll Packages"]},{"cell_type":"code","metadata":{"id":"cc5mjuYTiqrz","colab_type":"code","colab":{}},"source":["import string\n","import numpy as np\n","from PIL import Image\n","import os\n","from pickle import dump, load\n","import numpy as np\n","from keras.applications.xception import Xception, preprocess_input\n","from keras.preprocessing.image import load_img, img_to_array\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers.merge import add\n","from keras.models import Model, load_model\n","from keras.layers import Input, Dense, LSTM, Embedding, Dropout\n"," \n","from tqdm import tqdm_notebook as tqdm\n","tqdm().pandas()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SK9rfifQhCsQ","colab_type":"text"},"source":["# Data Cleaning"]},{"cell_type":"code","metadata":{"id":"kBwmhz1URY-W","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592464835352,"user_tz":-330,"elapsed":2141,"user":{"displayName":"HUNNURJI RAO KATIKA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjH10geXat-bUnVj4gm2nw-waQC3_V_D_oIdG2wlvw=s64","userId":"08627471873868781792"}}},"source":["def load_doc(filename):\n","    file = open(filename, 'r')\n","    text = file.read()\n","    file.close()\n","    return text"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"aPbYy3wMRZCu","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592464835357,"user_tz":-330,"elapsed":1592,"user":{"displayName":"HUNNURJI RAO KATIKA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjH10geXat-bUnVj4gm2nw-waQC3_V_D_oIdG2wlvw=s64","userId":"08627471873868781792"}}},"source":["def all_img_captions(filename):\n","    file = load_doc(filename)\n","    captions = file.split('\\n')\n","    descriptions ={}\n","    for caption in captions[:-1]:\n","        img, caption = caption.split('\\t')\n","        if img[:-2] not in descriptions:\n","            descriptions[img[:-2]] = list()\n","        else:\n","            descriptions[img[:-2]].append(caption)\n","    return descriptions"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"YcGaGpjYRZHu","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592464836193,"user_tz":-330,"elapsed":1738,"user":{"displayName":"HUNNURJI RAO KATIKA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjH10geXat-bUnVj4gm2nw-waQC3_V_D_oIdG2wlvw=s64","userId":"08627471873868781792"}}},"source":["def cleaning_text(captions):\n","    table = str.maketrans('','',string.punctuation)\n","    for img,caps in captions.items():\n","        for i,img_caption in enumerate(caps):\n","            img_caption.replace(\"-\",\" \")\n","            desc = img_caption.split()\n","            #converts to lowercase\n","            desc = [word.lower() for word in desc]\n","            #remove punctuation from each token\n","            desc = [word.translate(table) for word in desc]\n","            #remove hanging 's and a \n","            desc = [word for word in desc if(len(word)>1)]\n","            #remove tokens with numbers in them\n","            desc = [word for word in desc if(word.isalpha())]\n","            #convert back to string\n","            img_caption = ' '.join(desc)\n","            captions[img][i]= img_caption\n","    return captions"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"iNKrrn7dRhu4","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592464838755,"user_tz":-330,"elapsed":2112,"user":{"displayName":"HUNNURJI RAO KATIKA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjH10geXat-bUnVj4gm2nw-waQC3_V_D_oIdG2wlvw=s64","userId":"08627471873868781792"}}},"source":["def text_vocabulary(descriptions):\n","    # build vocabulary of all unique words\n","    vocab = set()\n","    for key in descriptions.keys():\n","        [vocab.update(d.split()) for d in descriptions[key]]\n","    return vocab"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"aZPnff2ERi6v","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592464840319,"user_tz":-330,"elapsed":1204,"user":{"displayName":"HUNNURJI RAO KATIKA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjH10geXat-bUnVj4gm2nw-waQC3_V_D_oIdG2wlvw=s64","userId":"08627471873868781792"}}},"source":["def save_descriptions(descriptions, filename):\n","    lines = list()\n","    for key, desc_list in descriptions.items():\n","        for desc in desc_list:\n","            lines.append(key + '\\t' + desc )\n","    data = \"\\n\".join(lines)\n","    file = open(filename,\"w\")\n","    file.write(data)\n","    file.close()"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hm0mGj3TiuMQ","colab_type":"code","colab":{}},"source":["dataset_images = \"Flicker8k_Dataset\"\n","\n","filename = \"Flickr8k.token.txt\"\n"," \n","descriptions = all_img_captions(filename)\n","print(\"Length of descriptions =\" ,len(descriptions))\n","\n","clean_descriptions = cleaning_text(descriptions)\n","  \n","vocabulary = text_vocabulary(clean_descriptions)\n","print(\"Length of vocabulary = \", len(vocabulary))\n"," \n","save_descriptions(clean_descriptions, \"descriptions.txt\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vNgRr0U0hLMA","colab_type":"text"},"source":["# Extracting all feature vectors from all images"]},{"cell_type":"code","metadata":{"id":"ObOBjDVx_8nu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":170,"referenced_widgets":["233f374cd0fa4987bdbe2e3710b30eba","583d49f038cf49bfb502e4a75767c899","e5e0e5d2809747e499a40a1f9f768f05","d5fb5e589d0f44bfaf7b79dd147a4aff","b4ba0f0a5cd045ecb0faba1cb25520c8","39a90fe32f244810a10e9d3437a99641","ca8e1f7b2af64d3991fbc2a833eb5cfc","75d7e5c8169349cab6b02cf5020debdc"]},"executionInfo":{"status":"ok","timestamp":1592465107096,"user_tz":-330,"elapsed":257725,"user":{"displayName":"HUNNURJI RAO KATIKA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjH10geXat-bUnVj4gm2nw-waQC3_V_D_oIdG2wlvw=s64","userId":"08627471873868781792"}},"outputId":"600f4204-833c-44e2-d519-dc56a2908633"},"source":["\n","def extract_features(directory):\n","        model = Xception( include_top=False, pooling='avg' )\n","        features = {}\n","        for img in tqdm(os.listdir(directory)):\n","            filename = directory + \"/\" + img\n","            image = Image.open(filename)\n","            image = image.resize((299,299))\n","            image = np.expand_dims(image, axis=0)\n","            #image = preprocess_input(image)\n","            image = image/127.5\n","            image = image - 1.0\n","            feature = model.predict(image)\n","            features[img] = feature\n","        return features\n","\n","\n","features = extract_features(dataset_images)\n","dump(features, open(\"features.p\",\"wb\"))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.4/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n","83689472/83683744 [==============================] - 4s 0us/step\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  \"\"\"\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"233f374cd0fa4987bdbe2e3710b30eba","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=8091.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Sh43oBc4ABbw","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592409889518,"user_tz":-330,"elapsed":2498,"user":{"displayName":"HUNNURJI RAO KATIKA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjH10geXat-bUnVj4gm2nw-waQC3_V_D_oIdG2wlvw=s64","userId":"08627471873868781792"}}},"source":["features = load(open(\"features.p\",\"rb\"))\n"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nOs-eRBUhc51","colab_type":"text"},"source":["# Loading the Datasets for training a model"]},{"cell_type":"code","metadata":{"id":"7VZiMlrASBsI","colab_type":"code","colab":{}},"source":["def load_photos(filename):\n","    file = load_doc(filename)\n","    photos = file.split(\"\\n\")[:-1]\n","    return photos"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F4Ik6OX4SBwH","colab_type":"code","colab":{}},"source":["def load_clean_descriptions(filename, photos): \n","    #loading clean_descriptions\n","    file = load_doc(filename)\n","    descriptions = {}\n","    for line in file.split(\"\\n\"):\n","        words = line.split()\n","        if len(words)<1 :\n","            continue\n","        image, image_caption = words[0], words[1:]\n","        if image in photos:\n","            if image not in descriptions:\n","                descriptions[image] = []\n","            desc = '<start> ' + \" \".join(image_caption) + ' <end>'\n","            descriptions[image].append(desc)\n","    return descriptions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ZMkbanvSBo4","colab_type":"code","colab":{}},"source":["def load_features(photos):\n","    #loading all features\n","    all_features = load(open(\"features.p\",\"rb\"))\n","    #selecting only needed features\n","    features = {k:all_features[k] for k in photos}\n","    return features"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JlvNSnlzSHwC","colab_type":"code","colab":{}},"source":["filename = \"Flickr_8k.trainImages.txt\"\n","\n","train_imgs = load_photos(filename)\n","\n","train_descriptions = load_clean_descriptions(\"descriptions.txt\", train_imgs)\n","\n","train_features = load_features(train_imgs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3XgkxZsRhptU","colab_type":"text"},"source":["#Tokenizing the Vocabulary"]},{"cell_type":"code","metadata":{"id":"saaxWzLMiz7R","colab_type":"code","colab":{}},"source":["def dict_to_list(descriptions):\n","    all_desc = []\n","    for key in descriptions.keys():\n","        [all_desc.append(d) for d in descriptions[key]]\n","    return all_desc\n"," \n","from keras.preprocessing.text import Tokenizer\n","def create_tokenizer(descriptions):\n","    desc_list = dict_to_list(descriptions)\n","    tokenizer = Tokenizer()\n","    tokenizer.fit_on_texts(desc_list)\n","    return tokenizer\n","\n","tokenizer = create_tokenizer(train_descriptions)\n","dump(tokenizer, open('tokenizer.p', 'wb'))\n","vocab_size = len(tokenizer.word_index) + 1\n","vocab_size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DiegtBb2i2gZ","colab_type":"code","colab":{}},"source":["def max_length(descriptions):\n","    desc_list = dict_to_list(descriptions)\n","    return max(len(d.split()) for d in desc_list)\n","    \n","max_length = max_length(descriptions)\n","max_length"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lyf5dvM4h9rF","colab_type":"text"},"source":["# Creating a Data Generator"]},{"cell_type":"code","metadata":{"id":"tSV5Z-E0SXjS","colab_type":"code","colab":{}},"source":["def data_generator(descriptions, features, tokenizer, max_length):\n","    while 1:\n","        for key, description_list in descriptions.items():\n","            #retrieve photo features\n","            feature = features[key][0]\n","            input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, description_list, feature)\n","            yield [[input_image, input_sequence], output_word]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3W8Sevi3SXm6","colab_type":"code","colab":{}},"source":["def create_sequences(tokenizer, max_length, desc_list, feature):\n","    X1, X2, y = list(), list(), list()\n","    # walk through each description for the image\n","    for desc in desc_list:\n","        # encode the sequence\n","        seq = tokenizer.texts_to_sequences([desc])[0]\n","        # split one sequence into multiple X,y pairs\n","        for i in range(1, len(seq)):\n","            # split into input and output pair\n","            in_seq, out_seq = seq[:i], seq[i]\n","            # pad input sequence\n","            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n","            # encode output sequence\n","            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n","            # store\n","            X1.append(feature)\n","            X2.append(in_seq)\n","            y.append(out_seq)\n","    return np.array(X1), np.array(X2), np.array(y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dyMglt-9SXhJ","colab_type":"code","colab":{}},"source":["[a,b],c = next(data_generator(train_descriptions, features, tokenizer, max_length))\n","a.shape, b.shape, c.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UDzHj7wwiE-I","colab_type":"text"},"source":["# Defining the CNN-RNN model"]},{"cell_type":"code","metadata":{"id":"THioT1M9BQDD","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592409925498,"user_tz":-330,"elapsed":1972,"user":{"displayName":"HUNNURJI RAO KATIKA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjH10geXat-bUnVj4gm2nw-waQC3_V_D_oIdG2wlvw=s64","userId":"08627471873868781792"}}},"source":["from keras.utils import plot_model\n","\n","def define_model(vocab_size, max_length):\n","    \n","    inputs1 = Input(shape=(2048,))\n","    fe1 = Dropout(0.5)(inputs1)\n","    fe2 = Dense(256, activation='relu')(fe1)\n","   \n","    inputs2 = Input(shape=(max_length,))\n","    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n","    se2 = Dropout(0.5)(se1)\n","    se3 = LSTM(256)(se2)\n","\n","    decoder1 = add([fe2, se3])\n","    decoder2 = Dense(256, activation='relu')(decoder1)\n","    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n","    \n","    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n","    model.compile(loss='categorical_crossentropy', optimizer='adam')\n","    \n","    print(model.summary())\n","    plot_model(model, to_file='model.png', show_shapes=True)\n","    return model"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z6t_MVmJiRwL","colab_type":"text"},"source":["# Training the Model"]},{"cell_type":"code","metadata":{"id":"g-Oj1EiZBQAd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":994},"executionInfo":{"status":"ok","timestamp":1592418347701,"user_tz":-330,"elapsed":8419981,"user":{"displayName":"HUNNURJI RAO KATIKA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjH10geXat-bUnVj4gm2nw-waQC3_V_D_oIdG2wlvw=s64","userId":"08627471873868781792"}},"outputId":"6b87df0e-2c30-4222-b889-d6821c832b89"},"source":["print('Dataset: ', len(train_imgs))\n","print('Descriptions: train=', len(train_descriptions))\n","print('Photos: train=', len(train_features))\n","print('Vocabulary Size:', vocab_size)\n","print('Description Length: ', max_length)\n","model = define_model(vocab_size, max_length)\n","epochs = 10\n","steps = len(train_descriptions)\n","\n","os.mkdir(\"models\")\n","for i in range(epochs):\n","    generator = data_generator(train_descriptions, train_features, tokenizer, max_length)\n","    model.fit_generator(generator, epochs=1, steps_per_epoch= steps, verbose=1)\n","\n","model.save(\"Image_captioning_model.h5\")"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Dataset:  6000\n","Descriptions: train= 6000\n","Photos: train= 6000\n","Vocabulary Size: 6889\n","Description Length:  32\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_3 (InputLayer)            (None, 32)           0                                            \n","__________________________________________________________________________________________________\n","input_2 (InputLayer)            (None, 2048)         0                                            \n","__________________________________________________________________________________________________\n","embedding_1 (Embedding)         (None, 32, 256)      1763584     input_3[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 2048)         0           input_2[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 32, 256)      0           embedding_1[0][0]                \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 256)          524544      dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","lstm_1 (LSTM)                   (None, 256)          525312      dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","add_13 (Add)                    (None, 256)          0           dense_1[0][0]                    \n","                                                                 lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 256)          65792       add_13[0][0]                     \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 6889)         1770473     dense_2[0][0]                    \n","==================================================================================================\n","Total params: 4,649,705\n","Trainable params: 4,649,705\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/1\n","6000/6000 [==============================] - 835s 139ms/step - loss: 4.5424\n","Epoch 1/1\n","6000/6000 [==============================] - 835s 139ms/step - loss: 3.6803\n","Epoch 1/1\n","6000/6000 [==============================] - 844s 141ms/step - loss: 3.3801\n","Epoch 1/1\n","6000/6000 [==============================] - 842s 140ms/step - loss: 3.2013\n","Epoch 1/1\n","6000/6000 [==============================] - 842s 140ms/step - loss: 3.0760\n","Epoch 1/1\n","6000/6000 [==============================] - 843s 141ms/step - loss: 2.9827\n","Epoch 1/1\n","6000/6000 [==============================] - 842s 140ms/step - loss: 2.9093\n","Epoch 1/1\n","6000/6000 [==============================] - 849s 141ms/step - loss: 2.8542\n","Epoch 1/1\n","6000/6000 [==============================] - 845s 141ms/step - loss: 2.8023\n","Epoch 1/1\n","6000/6000 [==============================] - 841s 140ms/step - loss: 2.7668\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"V1ya_SnNiOn1","colab_type":"text"},"source":["# Testing the Model"]},{"cell_type":"code","metadata":{"id":"6jNjRKCxi8Iq","colab_type":"code","colab":{}},"source":["import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","img_path = \" \" # enter image path here\n","\n","def extract_features(filename, model):\n","        try:\n","            image = Image.open(filename)\n","        except:\n","            print(\"ERROR: Couldn't open image! Make sure the image path and extension is correct\")\n","        image = image.resize((299,299))\n","        image = np.array(image)\n","        # for images that has 4 channels, we convert them into 3 channels\n","        if image.shape[2] == 4: \n","            image = image[..., :3]\n","        image = np.expand_dims(image, axis=0)\n","        image = image/127.5\n","        image = image - 1.0\n","        feature = model.predict(image)\n","        return feature\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ouK3uQ7nikmY","colab_type":"code","colab":{}},"source":["def word_for_id(integer, tokenizer):\n","  for word, index in tokenizer.word_index.items():\n","      if index == integer:\n","          return word\n","\n","def generate_desc(model, tokenizer, photo, max_length):\n","    in_text = 'start'\n","    for i in range(max_length):\n","        sequence = tokenizer.texts_to_sequences([in_text])[0]\n","        sequence = pad_sequences([sequence], maxlen=max_length)\n","        pred = model.predict([photo,sequence], verbose=0)\n","        pred = np.argmax(pred)\n","        word = word_for_id(pred, tokenizer)\n","        if word is None:\n","            break\n","        in_text += ' ' + word\n","        if word == 'end':\n","            break\n","    return in_text\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SxlgWAVujDSk","colab_type":"code","colab":{}},"source":["max_length = 32\n","tokenizer = load(open(\"tokenizer.p\",\"rb\"))\n","\n","model = load_model('Image_captioning_model.h5')\n","xception_model = Xception(include_top=False, pooling=\"avg\")\n","photo = extract_features(img_path, xception_model)\n","\n","img = Image.open(img_path)\n","description = generate_desc(model, tokenizer, photo, max_length)\n","description=description[6:-3] # removing start and end.\n","print(\"\\n\\n\")\n","print(description)\n","plt.imshow(img)"],"execution_count":null,"outputs":[]}]}